{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science: COVID-19 Data Analysis\n",
    "\n",
    "\n",
    "### Part 1: Development of COVID-19 Dashboard Prototype\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is one click walk through data gathering, process pipeline, filtering and doubling rate calculation. \n",
    "\n",
    "Utlimately, a COVID-19 Dashboard Prototype is created with the help of Plotly and DASH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check base directory**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]\n",
    "\n",
    "basedir = os.getcwd()\n",
    "basedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages and Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from scipy import signal\n",
    "\n",
    "import plotly.io as pio\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All Libraries Imported !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_johns_hopkins():\n",
    "    ''' Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure\n",
    "    '''\n",
    "    global basedir\n",
    "    print(\"You are in : \"+ str(os.getcwd()))\n",
    "\n",
    "    try:\n",
    "        os.chdir(\"data/raw/COVID-19/\")\n",
    "        print(\"Now you are in : \"+ str(os.getcwd()))\n",
    "        print(\"-------------- Great ! Repository is already present. Let's update it ---------------\")\n",
    "        !git pull https://github.com/CSSEGISandData/COVID-19.git\n",
    "    except FileNotFoundError:\n",
    "        os.chdir(\"data/raw/\")\n",
    "        print(\"Now you are in : \"+ str(os.getcwd()))\n",
    "        print(\"-------------- Let's initialize empty Git repository and clone it with Johns Hopkins repo --------------\")\n",
    "        !git init\n",
    "        !git clone https://github.com/CSSEGISandData/COVID-19.git\n",
    "    \n",
    "    os.chdir(basedir)\n",
    "    print(\"You are back in base directory : \" +str(os.getcwd()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_johns_hopkins()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset of 100 countries**\n",
    "\n",
    "Extracting Dataset of alphabetically 1st 100 countries, cleaning it and storing in a excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"..\\\\ads_covid-19\\\\data\\\\raw\\\\COVID-19\\\\csse_covid_19_data\\\\csse_covid_19_time_series\\\\time_series_covid19_deaths_global.csv\"\n",
    "df_all = pd.read_csv(data_path)\n",
    "  \n",
    "    \n",
    "time_idx=df_all.columns[4:]\n",
    "df_100 = pd.DataFrame({'date':time_idx})\n",
    "countries_to_consider = df_all['Country/Region'].unique().tolist()\n",
    "k = 1 # Counter for number of countries\n",
    "\n",
    "for each in countries_to_consider:\n",
    "    if k == 101:\n",
    "        break\n",
    "    df_100[each]=np.array(df_all[df_all['Country/Region']==each].iloc[:,4::].sum(axis=0))\n",
    "    k += 1\n",
    "\n",
    "time_idx=[datetime.strptime( each,\"%m/%d/%y\") for each in df_100.date] # convert to datetime\n",
    "time_str=[each.strftime('%Y-%m-%d') for each in time_idx] # convert back to date ISO norm (str)\n",
    "df_100['date']=time_idx\n",
    "df_100.to_csv('..\\\\ads_covid-19\\\\data\\\\processed\\\\COVID_100_countries_dataset_deaths.csv',index=False)\n",
    "df_100.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relational Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_relational_JH_data():\n",
    "    ''' Transformes the COVID data in a relational data set. Relational dataset is required for time series visualization\n",
    "\n",
    "    '''\n",
    "\n",
    "    data_path = \"..\\\\ads_covid-19\\\\data\\\\raw\\\\COVID-19\\\\csse_covid_19_data\\\\csse_covid_19_time_series\\\\time_series_covid19_deaths_global.csv\"\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "    df_data_base=df_raw.rename(columns={'Country/Region':'country','Province/State':'state'})\n",
    "    df_data_base['state']=df_data_base['state'].fillna('no')  #Substituting a fixed string at the places of NaN\n",
    "    df_data_base=df_data_base.drop(['Lat','Long'],axis=1) # Droping Latitude and Longitude axis\n",
    "    \n",
    "\n",
    "    df_relational_model=df_data_base.set_index(['state','country']).T.stack(level=[0,1]).reset_index().rename(columns={'level_0':'date', 0:'deaths'})\n",
    "    df_relational_model['date']=df_relational_model.date.astype('datetime64[ns]')\n",
    "    df_relational_model.confirmed=df_relational_model.deaths.astype(int)\n",
    "\n",
    "    df_relational_model.to_csv('..\\\\ads_covid-19\\\\data\\\\processed\\\\COVID_relational_deaths.csv',index=False)\n",
    "    print(' Number of rows stored: '+str(df_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(df_relational_model.date)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_JH_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter and Doubling Rate Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "    reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "    \n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "    \n",
    "    return intercept/slope\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Error in calc_filtered_data - not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['state','country']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    df_JH_data=pd.read_csv(\"..\\\\ads_covid-19\\\\data\\\\processed\\\\COVID_relational_confirmed.csv\",parse_dates=[0])\n",
    "    df_JH_data=df_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    #test_structure=pd_JH_data[((pd_JH_data['country']=='US')|\n",
    "    #                  (pd_JH_data['country']=='Germany'))]\n",
    "\n",
    "    df_result_larg=calc_filtered_data(df_JH_data)\n",
    "    df_result_larg=calc_doubling_rate(df_result_larg)\n",
    "    df_result_larg=calc_doubling_rate(df_result_larg,'confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=df_result_larg['confirmed']>100\n",
    "    df_result_larg['confirmed_filtered_DR']=df_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    df_result_larg.to_csv(\"..\\\\ads_covid-19\\\\data\\\\processed\\\\COVID_final_set.csv\",index=False)\n",
    "    print(df_result_larg[df_result_larg['country']=='India'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data for US\")\n",
    "print(df_result_larg[df_result_larg['country']=='US'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_larg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bhutan',\n",
       " 'Bolivia',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Botswana',\n",
       " 'Brazil',\n",
       " 'Brunei',\n",
       " 'Bulgaria',\n",
       " 'Burkina Faso',\n",
       " 'Burma',\n",
       " 'Burundi',\n",
       " 'Cabo Verde',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Central African Republic',\n",
       " 'Chad',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Colombia',\n",
       " 'Comoros',\n",
       " 'Congo (Brazzaville)',\n",
       " 'Congo (Kinshasa)',\n",
       " 'Costa Rica',\n",
       " \"Cote d'Ivoire\",\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Cyprus',\n",
       " 'Czechia',\n",
       " 'Denmark',\n",
       " 'Diamond Princess',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican Republic',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El Salvador',\n",
       " 'Equatorial Guinea',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Eswatini',\n",
       " 'Ethiopia',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Greece',\n",
       " 'Grenada',\n",
       " 'Guatemala',\n",
       " 'Guinea',\n",
       " 'Guinea-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Holy See',\n",
       " 'Honduras',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Iran',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Jordan',\n",
       " 'Kazakhstan',\n",
       " 'Kenya',\n",
       " 'Korea, South',\n",
       " 'Kosovo',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " 'Laos',\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_list = df_100.columns[1:].tolist()\n",
    "country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\ads_covid-19\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "df_input_large=pd.read_csv(\"..\\\\ads_covid-19\\\\data\\\\processed\\\\COVID_relational_deaths.csv\")\n",
    "\n",
    "fig = go.Figure()\n",
    "app = dash.Dash()\n",
    "\n",
    "app.layout = html.Div([\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    #  COVID-19 Data Visualization_Deaths\n",
    "    #### Completely automated data analysis process by Sucheta.\n",
    "\n",
    "    Goal of the project is to demonstrate data science concepts by applying a cross industry standard process.\n",
    "    \n",
    "    The data for this analysis is collected from Johns Hopkins University's Github repo.\n",
    "    \n",
    "    This project covers the full walkthrough of: Automated data pulling from John Hopkins University's Github repo, data cleaning and transforming, and ultimately loading the data in .csv file \n",
    "    \n",
    "    \n",
    "    Ultimately, this responsive dashboard is deployed.\n",
    "    \n",
    "    Future scope for this project will be implementing machine learning to approximating the doubling time.\n",
    "    \n",
    "\n",
    "    '''),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    ## Select Country for visualization\n",
    "    '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='country_drop_down',\n",
    "        options=[ {'label': each,'value':each} for each in df_100.columns[1:].tolist()],\n",
    "        value=['China', 'India'], # which are pre-selected\n",
    "        multi=True\n",
    "    ),\n",
    "\n",
    "    dcc.Graph(figure=fig, id='main_window_slope')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_drop_down', 'value')])\n",
    "def update_figure(country_list):\n",
    "    my_yaxis={'type':\"linear\",\n",
    "               'title':'Death rates'\n",
    "              }\n",
    "\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot=df_input_large[df_input_large['country']==each]\n",
    "\n",
    "        #print(show_doubling)\n",
    "\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[\"deaths\"],\n",
    "                                mode='markers+lines',\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "    \n",
    "    return {\n",
    "            'data': traces,\n",
    "            'layout': dict (\n",
    "                width=1280,\n",
    "                height=720,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
